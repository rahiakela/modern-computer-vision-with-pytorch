{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-semantic-segmentation-with-unet.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPsB9pyf0cTZftwKO6V/3fY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/modern-computer-vision-with-pytorch/blob/main/9-image-segmentation/1_semantic_segmentation_with_unet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZzy3qEZUOoH"
      },
      "source": [
        "## Semantic Segmentation with U-Net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSBdPeIHUecz"
      },
      "source": [
        "In this notebook, we will learn about semantic segmentation by taking a look at the U-Net architectures. Specifically, we will cover the following topics:\n",
        "\n",
        "- Exploring the U-Net architecture\n",
        "- Implementing semantic segmentation using U-Net\n",
        "\n",
        "A succinct image of what we are trying to achieve through image segmentation (https://arxiv.org/pdf/1405.0312.pdf) is as follows:\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/object-detection-images/image-segmentation.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8VsjMF0Voui"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sw7mwiZLVp_Y"
      },
      "source": [
        "!pip install -q torch_snippets pytorch_model_summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYOXwXuvV572"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_snippets import *\n",
        "from torchvision import transforms\n",
        "from torchvision.models import vgg16_bn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbDJNo6XWOjt"
      },
      "source": [
        "%%shell\n",
        "\n",
        "wget -q https://www.dropbox.com/s/0pigmmmynbf9xwq/dataset1.zip\n",
        "unzip -q dataset1.zip\n",
        "rm dataset1.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qf37F-IQVqUV"
      },
      "source": [
        "## Exploring the U-Net architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2FIJp8pVrL6"
      },
      "source": [
        "Imagine a scenario where you've been given an image and been asked to predict\n",
        "which pixel corresponds to what object. \n",
        "\n",
        "So far, when we have been predicting the class of an object and the bounding box corresponding to the object, we passed the image through a network, which then passes the image through a backbone architecture (such as VGG or ResNet), flattens the output at a certain layer, and connects additional dense layers before making predictions for the class and bounding box offsets. \n",
        "\n",
        "However, in the case of image segmentation, where the output shape is the same as that of the input image's shape, flattening the convolutions' outputs and then reconstructing the image might result in a loss of information.\n",
        "\n",
        "Furthermore, the contours and shapes present in the original image will not vary in the output image in the case of image segmentation, so the networks we have dealt with so far (which flatten the last layer and connect additional dense layers) are not optimal when we are performing segmentation.\n",
        "\n",
        "The two aspects that we need to keep in mind while performing segmentation are as follows:\n",
        "\n",
        "- The shape and structure of the objects in the original image remain the\n",
        "same in the segmented output.\n",
        "- Leveraging a fully convolutional architecture (and not a structure where\n",
        "we flatten a certain layer) can help here since we are using one image as\n",
        "input and another as output.\n",
        "\n",
        "The U-Net architecture helps us achieve this. A typical representation of U-Net is as follows (the input image is of the shape `3 x 96 x 128`, while the number of classes present in the image is 21; this means that the output contains 21 channels):\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/object-detection-images/unet.png?raw=1' width='800'/>\n",
        "\n",
        "**The preceding architecture is called a U-Net architecture because of its \"U\"-like shape.**\n",
        "\n",
        "In the left half of the preceding diagram, we can see that the image passes through convolution layers, and that the image size keeps reducing while the number of channels keeps increasing. However, in the right half, we can see that we are upscaling the downscaled image, back to the original height and width but with as many channels as there are classes.\n",
        "\n",
        "**In addition, while upscaling, we are also leveraging information from the\n",
        "corresponding layers in the left half using skip connections so that we can preserve the structure/objects in the original image.**\n",
        "\n",
        "**This way, the U-Net architecture learns to preserve the structure (and shapes of objects) of the original image while leveraging the convolution's features to predict the classes that correspond to each pixel.**\n",
        "\n",
        "In general, we have as many channels in the output as the number of classes we want to predict."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XNHm_LRY5D3"
      },
      "source": [
        "### Performing upscaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QHLI_0RY6IS"
      },
      "source": [
        "In the U-Net architecture, upscaling is performed using the nn.ConvTranspose2d\n",
        "method, which takes the number of input channels, the number of output channels, the kernel size, and stride as input parameters.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/object-detection-images/upscaling.png?raw=1' width='800'/>\n",
        "\n",
        "In the preceding example, we took an input array of shape `3 x 3` (Input array), applied a stride of 2 where we distributed the input values to accommodate the stride (Input array adjusted for stride), padded the array with zeros (Input array adjusted for stride and padding), and convolved the padded input with a filter (Filter/Kernel) to fetch the output array.\n",
        "\n",
        "In order to understand how nn.ConvTranspose2d helps upscale an array, let's go\n",
        "through the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwQEoX0PZu1x"
      },
      "source": [
        "# Initialize a network, m, with the nn.ConvTranspose2d method by specifying input channel's value is 1, output channel's value is 1\n",
        "m = nn.ConvTranspose2d(1, 1, kernel_size=(2, 2), stride=2, padding=0)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTksoPgIcXcZ"
      },
      "source": [
        "Internally, padding is calculated as `dilation * (kernel_size - 1) - padding`.\n",
        "\n",
        "Hence `1*(2-1)-0 = 1`, where we add zero padding of 1 to both dimensions of\n",
        "the input array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "id": "5-CK97ezcAo-",
        "outputId": "7804e9eb-ad78-4855-b773-5c8dea4867c8"
      },
      "source": [
        "# Initialize an input array and pass it through the model\n",
        "input = torch.ones(1, 1, 3, 3)\n",
        "output = m(input)\n",
        "print(output.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">torch.Size<span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span><span style=\"font-weight: bold\">])</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "torch.Size\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m6\u001b[0m, \u001b[1;36m6\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl8KXoJmcreE",
        "outputId": "51c4d368-338f-45e8-d737-302260c03664"
      },
      "source": [
        "output"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ 0.5472, -0.1862,  0.5472, -0.1862,  0.5472, -0.1862],\n",
              "          [-0.0158,  0.4669, -0.0158,  0.4669, -0.0158,  0.4669],\n",
              "          [ 0.5472, -0.1862,  0.5472, -0.1862,  0.5472, -0.1862],\n",
              "          [-0.0158,  0.4669, -0.0158,  0.4669, -0.0158,  0.4669],\n",
              "          [ 0.5472, -0.1862,  0.5472, -0.1862,  0.5472, -0.1862],\n",
              "          [-0.0158,  0.4669, -0.0158,  0.4669, -0.0158,  0.4669]]]],\n",
              "       grad_fn=<SlowConvTranspose2DBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzbMAbGvdPB_"
      },
      "source": [
        "## Implementing semantic segmentation using U-Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BRzDGVGc9d9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}