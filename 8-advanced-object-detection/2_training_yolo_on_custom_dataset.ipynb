{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-training-yolo-on-custom-dataset.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOTf1UqyK/+7yxEpDUxdmXZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/modern-computer-vision-with-pytorch/blob/main/8-advanced-object-detection/2_training_yolo_on_custom_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7lP-JfPPH6Q"
      },
      "source": [
        "## Training YOLO on a custom dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hq5UOinwQ3yw"
      },
      "source": [
        "You Only Look Once (YOLO) and its variants are one of the prominent object\r\n",
        "detection algorithms.\r\n",
        "\r\n",
        "that are likely to contain an object, and then we make the bounding box corrections. However, in the fully connected layer, where only the detected region's RoI pooling output is passed as input, in the case of regions that do not fully encompass the object(where the object is beyond the boundaries of the bounding box of region proposal), the network has to guess the real boundaries of object, as it has not seen the full image(but has seen only the region proposal).\r\n",
        "\r\n",
        "**YOLO comes in handy in such scenarios, as it looks at the whole image while\r\n",
        "predicting the bounding box corresponding to an image.**\r\n",
        "\r\n",
        "Furthermore, Faster R-CNN is still slow, as we have two networks: the RPN and the final network that predicts classes and bounding boxes around objects.\r\n",
        "\r\n",
        "YOLO overcomes the limitations of Faster R-CNN, both by looking at the whole image at once as well as by having a single network to make predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40020-nBRi3o"
      },
      "source": [
        "## Working details of YOLO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHWGkOasRm_B"
      },
      "source": [
        "We will look at how data is prepared for YOLO through the\r\n",
        "following example:\r\n",
        "\r\n",
        "**Step-1: Create a ground truth to train a model for a given image:**\r\n",
        "\r\n",
        "Let's consider an image with the given ground truth of bounding\r\n",
        "boxes in red:\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/object-detection-images/bounding-box-1.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "- Divide the image into $N x N$ grid cells – for now, let's say $N=3$:\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/object-detection-images/bounding-box-2.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "- Identify those grid cells that contain the center of at least one ground\r\n",
        "truth bounding box. In our case, they are cells $b1$ and $b3$ of our 3 x 3\r\n",
        "grid image.\r\n",
        "\r\n",
        "- The cell(s) where the middle point of ground truth bounding box falls\r\n",
        "is/are responsible for predicting the bounding box of the object. Let's\r\n",
        "create the ground truth corresponding to each cell.\r\n",
        "\r\n",
        "- The output ground truth corresponding to each cell is as follows:\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/object-detection-images/bounding-box-3.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "Here, pc (the objectness score) is the probability of the cell containing\r\n",
        "an object.\r\n",
        "\r\n",
        "Let's understand how to calculate bx, by, bw, and bh.\r\n",
        "\r\n",
        "First, we consider the grid cell (let's consider the b1 grid cell) as our\r\n",
        "universe, and normalize it to a scale between 0 and 1, as follows:\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/object-detection-images/bounding-box-4.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "bx and by are the locations of the mid-point of the ground truth\r\n",
        "bounding box with respect to the image (of the grid cell), as defined\r\n",
        "previously. In our case, bx = 0.5, as the mid-point of the ground truth is\r\n",
        "at a distance of 0.5 units from the origin. Similarly, by= 0.5:\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/object-detection-images/bounding-box-5.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "So far, we have calculated offsets from the grid cell center to the\r\n",
        "ground truth center corresponding to the object in the image. Now,\r\n",
        "let's understand how bw and bh are calculated.\r\n",
        "\r\n",
        "**bw is the ratio of the width of the bounding box with respect to the the\r\n",
        "width of the grid cell.**\r\n",
        "\r\n",
        "**bh is the ratio of the height of the bounding box with respect to the\r\n",
        "height of the grid cell.**\r\n",
        "\r\n",
        "Next, we will predict the class corresponding to the grid cell. If we\r\n",
        "have three classes (c1 – truck, c2 – car, c3 – bus), we will predict the\r\n",
        "probability of the cell containing an object among any of the three\r\n",
        "classes. **Note that we do not need a background class here, as pc\r\n",
        "corresponds to whether the grid cell contains an object.**\r\n",
        "\r\n",
        "Now that we understand how to represent the output layer of each cell,\r\n",
        "let's understand how we construct the output of our 3 x 3 grid cells.\r\n",
        "\r\n",
        "- Let's consider the output of the grid cell a3:\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/object-detection-images/bounding-box-6.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "As the grid cell does not contain an object, the first output (pc – objectness\r\n",
        "score) is 0 and the remaining values do not matter as the cell does not\r\n",
        "contain the center of any ground truth bounding boxes of an object.\r\n",
        "\r\n",
        "- Let's consider the output corresponding to grid cell b1:\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/object-detection-images/bounding-box-7.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "The preceding output is the way it is because the grid cell contains an\r\n",
        "object with the bx, by, bw, and bh values that were obtained in the\r\n",
        "same way as we went through earlier (in the bullet point before last),\r\n",
        "and finally the class being car resulting in c2 being 1 while c1 and c3\r\n",
        "are 0.\r\n",
        "\r\n",
        "**Note that for each cell, we are able to fetch 8 outputs. Hence, for the 3 x\r\n",
        "3 grid of cells, we fetch 3 x 3 x 8 outputs.**\r\n",
        "\r\n",
        "**Step-2: Define a model where the input is an image and the output is 3 x 3 x 8 with the ground truth being as defined in the previous step:**\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/object-detection-images/bounding-box-8.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "**Step-3:Define the ground truth by considering the anchor boxes.**\r\n",
        "\r\n",
        "So far, we have been building for a scenario where the expectation is that\r\n",
        "there is only one object within a grid cell. However, in reality, there can be\r\n",
        "scenarios where there are multiple objects within the same grid cell. This\r\n",
        "would result in creating ground truths that are incorrect. \r\n",
        "\r\n",
        "Let's understand this phenomenon through the following example image:\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/object-detection-images/bounding-box-9.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "In the preceding example, the mid-point of the ground truth bounding\r\n",
        "boxes for both the car and the person fall in the same cell – cell b1.\r\n",
        "\r\n",
        "One way to avoid such a scenario is by having a grid that has more rows\r\n",
        "and columns – for example, a 19 x 19 grid. However, there can still be a\r\n",
        "scenario where an increase in the number of grid cells does not help.\r\n",
        "**Anchor boxes come in handy in such a scenario.** Let's say we have two\r\n",
        "anchor boxes – one that has a greater height than width (corresponding to\r\n",
        "the person) and another that has a greater width than height (corresponding\r\n",
        "to the car):\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/object-detection-images/bounding-box-10.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "Typically, the anchor boxes would have the grid cell center as their centers.\r\n",
        "The output for each cell in a scenario where we have two anchor boxes is\r\n",
        "represented as a concatenation of the output expected of the two anchor\r\n",
        "boxes:\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/object-detection-images/bounding-box-11.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "Here, bx, by, bw, and bh represent the offset from the anchor box (which is\r\n",
        "the universe in this scenario as seen in the image instead of the grid cell).\r\n",
        "\r\n",
        "From the preceding screenshot, we see we have an output that is 3 x 3 x 16,\r\n",
        "as we have two anchors. The expected output is of the shape N x N x\r\n",
        "(num_classes + 1) x (num_anchor_boxes), where N x N is the number of\r\n",
        "cells in the grid, num_classes is the number of classes in the dataset, and\r\n",
        "num_anchor_boxes is the number of anchor boxes.\r\n",
        "\r\n",
        "**Step-4:Now we define the loss function to train the model.**\r\n",
        "\r\n",
        "When calculating the loss associated with the model, we need to ensure that\r\n",
        "we do not calculate the regression loss and classification loss when the\r\n",
        "objectness score is less than a certain threshold (this corresponds to the cells\r\n",
        "that do not contain an object).\r\n",
        "\r\n",
        "Next, if the cell contains an object, we need to ensure that the classification\r\n",
        "across different classes is as accurate as possible.\r\n",
        "\r\n",
        "Finally, if the cell contains an object, the bounding box offsets should be as\r\n",
        "close to expected as possible. However, since the offsets of width and height\r\n",
        "can be much higher when compared to the offset of the center (as offsets of\r\n",
        "the center range between 0 and 1, while the offsets of width and height need\r\n",
        "not), we give a lower weightage to offsets of width and height by fetching a\r\n",
        "square root value.\r\n",
        "\r\n",
        "Calculate the loss of localization and classification as follows:\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/object-detection-images/yolo-equation.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "**The overall loss is a sum of classification and regression loss values.**\r\n",
        "\r\n",
        "With this in place, we are now in a position to train a model to predict the bounding boxes around objects.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOm-s4BVccK-"
      },
      "source": [
        "## Training YOLO on a custom dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bCwSRd0ce86"
      },
      "source": [
        ""
      ]
    }
  ]
}