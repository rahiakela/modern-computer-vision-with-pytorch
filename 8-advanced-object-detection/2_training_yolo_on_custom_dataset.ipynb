{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-training-yolo-on-custom-dataset.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyObw7MC+2fi7NhZvRLEqNiE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/modern-computer-vision-with-pytorch/blob/main/8-advanced-object-detection/2_training_yolo_on_custom_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7lP-JfPPH6Q"
      },
      "source": [
        "## Training YOLO on a custom dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hq5UOinwQ3yw"
      },
      "source": [
        "You Only Look Once (YOLO) and its variants are one of the prominent object\r\n",
        "detection algorithms.\r\n",
        "\r\n",
        "that are likely to contain an object, and then we make the bounding box corrections. However, in the fully connected layer, where only the detected region's RoI pooling output is passed as input, in the case of regions that do not fully encompass the object(where the object is beyond the boundaries of the bounding box of region proposal), the network has to guess the real boundaries of object, as it has not seen the full image(but has seen only the region proposal).\r\n",
        "\r\n",
        "**YOLO comes in handy in such scenarios, as it looks at the whole image while\r\n",
        "predicting the bounding box corresponding to an image.**\r\n",
        "\r\n",
        "Furthermore, Faster R-CNN is still slow, as we have two networks: the RPN and the final network that predicts classes and bounding boxes around objects.\r\n",
        "\r\n",
        "YOLO overcomes the limitations of Faster R-CNN, both by looking at the whole image at once as well as by having a single network to make predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40020-nBRi3o"
      },
      "source": [
        "## Working details of YOLO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHWGkOasRm_B"
      },
      "source": [
        "We will look at how data is prepared for YOLO through the\r\n",
        "following example:\r\n",
        "\r\n",
        "**Step-1: Create a ground truth to train a model for a given image:**\r\n",
        "\r\n",
        "Let's consider an image with the given ground truth of bounding\r\n",
        "boxes in red:\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/object-detection-images/bounding-box-1.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "- Divide the image into $N x N$ grid cells – for now, let's say $N=3$:\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/object-detection-images/bounding-box-2.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "- Identify those grid cells that contain the center of at least one ground\r\n",
        "truth bounding box. In our case, they are cells $b1$ and $b3$ of our 3 x 3\r\n",
        "grid image.\r\n",
        "\r\n",
        "- The cell(s) where the middle point of ground truth bounding box falls\r\n",
        "is/are responsible for predicting the bounding box of the object. Let's\r\n",
        "create the ground truth corresponding to each cell.\r\n",
        "\r\n",
        "- The output ground truth corresponding to each cell is as follows:\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/object-detection-images/bounding-box-3.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "Here, pc (the objectness score) is the probability of the cell containing\r\n",
        "an object.\r\n",
        "\r\n",
        "Let's understand how to calculate bx, by, bw, and bh.\r\n",
        "\r\n",
        "First, we consider the grid cell (let's consider the b1 grid cell) as our\r\n",
        "universe, and normalize it to a scale between 0 and 1, as follows:\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/object-detection-images/bounding-box-4.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "bx and by are the locations of the mid-point of the ground truth\r\n",
        "bounding box with respect to the image (of the grid cell), as defined\r\n",
        "previously. In our case, bx = 0.5, as the mid-point of the ground truth is\r\n",
        "at a distance of 0.5 units from the origin. Similarly, by= 0.5:\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/object-detection-images/bounding-box-5.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "So far, we have calculated offsets from the grid cell center to the\r\n",
        "ground truth center corresponding to the object in the image. Now,\r\n",
        "let's understand how bw and bh are calculated.\r\n",
        "\r\n",
        "**bw is the ratio of the width of the bounding box with respect to the the\r\n",
        "width of the grid cell.**\r\n",
        "\r\n",
        "**bh is the ratio of the height of the bounding box with respect to the\r\n",
        "height of the grid cell.**\r\n",
        "\r\n",
        "Next, we will predict the class corresponding to the grid cell. If we\r\n",
        "have three classes (c1 – truck, c2 – car, c3 – bus), we will predict the\r\n",
        "probability of the cell containing an object among any of the three\r\n",
        "classes. **Note that we do not need a background class here, as pc\r\n",
        "corresponds to whether the grid cell contains an object.**\r\n",
        "\r\n",
        "Now that we understand how to represent the output layer of each cell,\r\n",
        "let's understand how we construct the output of our 3 x 3 grid cells.\r\n",
        "\r\n",
        "- Let's consider the output of the grid cell a3:\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/object-detection-images/bounding-box-6.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "As the grid cell does not contain an object, the first output (pc – objectness\r\n",
        "score) is 0 and the remaining values do not matter as the cell does not\r\n",
        "contain the center of any ground truth bounding boxes of an object.\r\n",
        "\r\n",
        "- Let's consider the output corresponding to grid cell b1:\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/object-detection-images/bounding-box-7.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "The preceding output is the way it is because the grid cell contains an\r\n",
        "object with the bx, by, bw, and bh values that were obtained in the\r\n",
        "same way as we went through earlier (in the bullet point before last),\r\n",
        "and finally the class being car resulting in c2 being 1 while c1 and c3\r\n",
        "are 0.\r\n",
        "\r\n",
        "**Note that for each cell, we are able to fetch 8 outputs. Hence, for the 3 x\r\n",
        "3 grid of cells, we fetch 3 x 3 x 8 outputs.**\r\n",
        "\r\n",
        "**Step-2: Define a model where the input is an image and the output is 3 x 3 x 8 with the ground truth being as defined in the previous step:**\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/object-detection-images/bounding-box-8.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "**Step-3:Define the ground truth by considering the anchor boxes.**\r\n",
        "\r\n",
        "So far, we have been building for a scenario where the expectation is that\r\n",
        "there is only one object within a grid cell. However, in reality, there can be\r\n",
        "scenarios where there are multiple objects within the same grid cell. This\r\n",
        "would result in creating ground truths that are incorrect. \r\n",
        "\r\n",
        "Let's understand this phenomenon through the following example image:\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/object-detection-images/bounding-box-9.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "In the preceding example, the mid-point of the ground truth bounding\r\n",
        "boxes for both the car and the person fall in the same cell – cell b1.\r\n",
        "\r\n",
        "One way to avoid such a scenario is by having a grid that has more rows\r\n",
        "and columns – for example, a 19 x 19 grid. However, there can still be a\r\n",
        "scenario where an increase in the number of grid cells does not help.\r\n",
        "**Anchor boxes come in handy in such a scenario.** Let's say we have two\r\n",
        "anchor boxes – one that has a greater height than width (corresponding to\r\n",
        "the person) and another that has a greater width than height (corresponding\r\n",
        "to the car):\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/object-detection-images/bounding-box-10.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "Typically, the anchor boxes would have the grid cell center as their centers.\r\n",
        "The output for each cell in a scenario where we have two anchor boxes is\r\n",
        "represented as a concatenation of the output expected of the two anchor\r\n",
        "boxes:\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/object-detection-images/bounding-box-11.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "Here, bx, by, bw, and bh represent the offset from the anchor box (which is\r\n",
        "the universe in this scenario as seen in the image instead of the grid cell).\r\n",
        "\r\n",
        "From the preceding screenshot, we see we have an output that is 3 x 3 x 16,\r\n",
        "as we have two anchors. The expected output is of the shape N x N x\r\n",
        "(num_classes + 1) x (num_anchor_boxes), where N x N is the number of\r\n",
        "cells in the grid, num_classes is the number of classes in the dataset, and\r\n",
        "num_anchor_boxes is the number of anchor boxes.\r\n",
        "\r\n",
        "**Step-4:Now we define the loss function to train the model.**\r\n",
        "\r\n",
        "When calculating the loss associated with the model, we need to ensure that\r\n",
        "we do not calculate the regression loss and classification loss when the\r\n",
        "objectness score is less than a certain threshold (this corresponds to the cells\r\n",
        "that do not contain an object).\r\n",
        "\r\n",
        "Next, if the cell contains an object, we need to ensure that the classification\r\n",
        "across different classes is as accurate as possible.\r\n",
        "\r\n",
        "Finally, if the cell contains an object, the bounding box offsets should be as\r\n",
        "close to expected as possible. However, since the offsets of width and height\r\n",
        "can be much higher when compared to the offset of the center (as offsets of\r\n",
        "the center range between 0 and 1, while the offsets of width and height need\r\n",
        "not), we give a lower weightage to offsets of width and height by fetching a\r\n",
        "square root value.\r\n",
        "\r\n",
        "Calculate the loss of localization and classification as follows:\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/object-detection-images/yolo-equation.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "**The overall loss is a sum of classification and regression loss values.**\r\n",
        "\r\n",
        "With this in place, we are now in a position to train a model to predict the bounding boxes around objects.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOm-s4BVccK-"
      },
      "source": [
        "## Training YOLO on a custom dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bCwSRd0ce86"
      },
      "source": [
        "Building on top of others' work is very important to becoming a successful\r\n",
        "practitioner in deep learning. For this implementation, we will use the official YOLOv4 implementation to identify the location of buses and trucks in images. We will clone the repository of the authors' own implementation of YOLO and customize it to our needs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1KLM_BZqJLD"
      },
      "source": [
        "### Installing Darknet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl03UyNeqKY_"
      },
      "source": [
        "First, pull the darknet repository from GitHub and compile it in the environment. The model is written in a separate language called Darknet, which is different from PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7foFRGjCuY6_"
      },
      "source": [
        "!rm -rf darknet"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVyEoFHDpe4g",
        "outputId": "a912116b-d027-45df-81ca-9045416749a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Pull the Git repo\r\n",
        "!git clone https://github.com/AlexeyAB/darknet"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'darknet'...\n",
            "remote: Enumerating objects: 14730, done.\u001b[K\n",
            "remote: Total 14730 (delta 0), reused 0 (delta 0), pack-reused 14730\u001b[K\n",
            "Receiving objects: 100% (14730/14730), 13.27 MiB | 24.14 MiB/s, done.\n",
            "Resolving deltas: 100% (10020/10020), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLYfOWdXy5qA",
        "outputId": "31ea731e-5b4b-4570-fab9-c3ad721a5186",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd /content/darknet   # it make working directory\r\n",
        "\r\n",
        "# Reconfigure the Makefile file\r\n",
        "!sed -i 's/OPENCV=0/OPENCV=1/' Makefile\r\n",
        "# !!! In case you dont have a GPU, make sure to comment out the below 3 lines !!! #\r\n",
        "!sed -i 's/GPU=0/GPU=1/' Makefile\r\n",
        "!sed -i 's/CUDNN=0/CUDNN=1/' Makefile\r\n",
        "!sed -i 's/CUDNN_HALF=0/CUDNN_HALF=1/' Makefile"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/darknet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfXO2lHewgKO"
      },
      "source": [
        "# Compile the darknet source code\r\n",
        "!make"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmZuzs4Nzb9Q",
        "outputId": "0c0572fa-99cc-4152-c088-f06d65510e04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%shell\r\n",
        "\r\n",
        "# Install the torch_snippets package\r\n",
        "pip install -q torch_snippets\r\n",
        "\r\n",
        "# Download and extract the dataset, and remove the ZIP file to save space\r\n",
        "wget --quiet https://www.dropbox.com/s/agmzwk95v96ihic/open-images-bus-trucks.tar.xz\r\n",
        "tar -xf open-images-bus-trucks.tar.xz\r\n",
        "rm open-images-bus-trucks.tar.xz\r\n",
        "\r\n",
        "# Fetch the pre-trained weights to make a sample prediction\r\n",
        "wget --quiet https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YATsIohr-Un"
      },
      "source": [
        "Let's test whether the installation is successful by running the following\r\n",
        "command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZAeRe2ArU-V",
        "outputId": "b136d6c5-90b4-4206-b57a-c37c43b1d1ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!./darknet detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights data/person.jpg"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " CUDA-version: 11000 (11020), cuDNN: 7.6.5, CUDNN_HALF=1, GPU count: 1  \n",
            " CUDNN_HALF=1 \n",
            " OpenCV version: 3.2.0\n",
            " 0 : compute_capability = 750, cudnn_half = 1, GPU: Tesla T4 \n",
            "net.optimized_memory = 0 \n",
            "mini_batch = 1, batch = 8, time_steps = 1, train = 0 \n",
            "   layer   filters  size/strd(dil)      input                output\n",
            "   0 Create CUDA-stream - 0 \n",
            " Create cudnn-handle 0 \n",
            "conv     32       3 x 3/ 1    608 x 608 x   3 ->  608 x 608 x  32 0.639 BF\n",
            "   1 conv     64       3 x 3/ 2    608 x 608 x  32 ->  304 x 304 x  64 3.407 BF\n",
            "   2 conv     64       1 x 1/ 1    304 x 304 x  64 ->  304 x 304 x  64 0.757 BF\n",
            "   3 route  1 \t\t                           ->  304 x 304 x  64 \n",
            "   4 conv     64       1 x 1/ 1    304 x 304 x  64 ->  304 x 304 x  64 0.757 BF\n",
            "   5 conv     32       1 x 1/ 1    304 x 304 x  64 ->  304 x 304 x  32 0.379 BF\n",
            "   6 conv     64       3 x 3/ 1    304 x 304 x  32 ->  304 x 304 x  64 3.407 BF\n",
            "   7 Shortcut Layer: 4,  wt = 0, wn = 0, outputs: 304 x 304 x  64 0.006 BF\n",
            "   8 conv     64       1 x 1/ 1    304 x 304 x  64 ->  304 x 304 x  64 0.757 BF\n",
            "   9 route  8 2 \t                           ->  304 x 304 x 128 \n",
            "  10 conv     64       1 x 1/ 1    304 x 304 x 128 ->  304 x 304 x  64 1.514 BF\n",
            "  11 conv    128       3 x 3/ 2    304 x 304 x  64 ->  152 x 152 x 128 3.407 BF\n",
            "  12 conv     64       1 x 1/ 1    152 x 152 x 128 ->  152 x 152 x  64 0.379 BF\n",
            "  13 route  11 \t\t                           ->  152 x 152 x 128 \n",
            "  14 conv     64       1 x 1/ 1    152 x 152 x 128 ->  152 x 152 x  64 0.379 BF\n",
            "  15 conv     64       1 x 1/ 1    152 x 152 x  64 ->  152 x 152 x  64 0.189 BF\n",
            "  16 conv     64       3 x 3/ 1    152 x 152 x  64 ->  152 x 152 x  64 1.703 BF\n",
            "  17 Shortcut Layer: 14,  wt = 0, wn = 0, outputs: 152 x 152 x  64 0.001 BF\n",
            "  18 conv     64       1 x 1/ 1    152 x 152 x  64 ->  152 x 152 x  64 0.189 BF\n",
            "  19 conv     64       3 x 3/ 1    152 x 152 x  64 ->  152 x 152 x  64 1.703 BF\n",
            "  20 Shortcut Layer: 17,  wt = 0, wn = 0, outputs: 152 x 152 x  64 0.001 BF\n",
            "  21 conv     64       1 x 1/ 1    152 x 152 x  64 ->  152 x 152 x  64 0.189 BF\n",
            "  22 route  21 12 \t                           ->  152 x 152 x 128 \n",
            "  23 conv    128       1 x 1/ 1    152 x 152 x 128 ->  152 x 152 x 128 0.757 BF\n",
            "  24 conv    256       3 x 3/ 2    152 x 152 x 128 ->   76 x  76 x 256 3.407 BF\n",
            "  25 conv    128       1 x 1/ 1     76 x  76 x 256 ->   76 x  76 x 128 0.379 BF\n",
            "  26 route  24 \t\t                           ->   76 x  76 x 256 \n",
            "  27 conv    128       1 x 1/ 1     76 x  76 x 256 ->   76 x  76 x 128 0.379 BF\n",
            "  28 conv    128       1 x 1/ 1     76 x  76 x 128 ->   76 x  76 x 128 0.189 BF\n",
            "  29 conv    128       3 x 3/ 1     76 x  76 x 128 ->   76 x  76 x 128 1.703 BF\n",
            "  30 Shortcut Layer: 27,  wt = 0, wn = 0, outputs:  76 x  76 x 128 0.001 BF\n",
            "  31 conv    128       1 x 1/ 1     76 x  76 x 128 ->   76 x  76 x 128 0.189 BF\n",
            "  32 conv    128       3 x 3/ 1     76 x  76 x 128 ->   76 x  76 x 128 1.703 BF\n",
            "  33 Shortcut Layer: 30,  wt = 0, wn = 0, outputs:  76 x  76 x 128 0.001 BF\n",
            "  34 conv    128       1 x 1/ 1     76 x  76 x 128 ->   76 x  76 x 128 0.189 BF\n",
            "  35 conv    128       3 x 3/ 1     76 x  76 x 128 ->   76 x  76 x 128 1.703 BF\n",
            "  36 Shortcut Layer: 33,  wt = 0, wn = 0, outputs:  76 x  76 x 128 0.001 BF\n",
            "  37 conv    128       1 x 1/ 1     76 x  76 x 128 ->   76 x  76 x 128 0.189 BF\n",
            "  38 conv    128       3 x 3/ 1     76 x  76 x 128 ->   76 x  76 x 128 1.703 BF\n",
            "  39 Shortcut Layer: 36,  wt = 0, wn = 0, outputs:  76 x  76 x 128 0.001 BF\n",
            "  40 conv    128       1 x 1/ 1     76 x  76 x 128 ->   76 x  76 x 128 0.189 BF\n",
            "  41 conv    128       3 x 3/ 1     76 x  76 x 128 ->   76 x  76 x 128 1.703 BF\n",
            "  42 Shortcut Layer: 39,  wt = 0, wn = 0, outputs:  76 x  76 x 128 0.001 BF\n",
            "  43 conv    128       1 x 1/ 1     76 x  76 x 128 ->   76 x  76 x 128 0.189 BF\n",
            "  44 conv    128       3 x 3/ 1     76 x  76 x 128 ->   76 x  76 x 128 1.703 BF\n",
            "  45 Shortcut Layer: 42,  wt = 0, wn = 0, outputs:  76 x  76 x 128 0.001 BF\n",
            "  46 conv    128       1 x 1/ 1     76 x  76 x 128 ->   76 x  76 x 128 0.189 BF\n",
            "  47 conv    128       3 x 3/ 1     76 x  76 x 128 ->   76 x  76 x 128 1.703 BF\n",
            "  48 Shortcut Layer: 45,  wt = 0, wn = 0, outputs:  76 x  76 x 128 0.001 BF\n",
            "  49 conv    128       1 x 1/ 1     76 x  76 x 128 ->   76 x  76 x 128 0.189 BF\n",
            "  50 conv    128       3 x 3/ 1     76 x  76 x 128 ->   76 x  76 x 128 1.703 BF\n",
            "  51 Shortcut Layer: 48,  wt = 0, wn = 0, outputs:  76 x  76 x 128 0.001 BF\n",
            "  52 conv    128       1 x 1/ 1     76 x  76 x 128 ->   76 x  76 x 128 0.189 BF\n",
            "  53 route  52 25 \t                           ->   76 x  76 x 256 \n",
            "  54 conv    256       1 x 1/ 1     76 x  76 x 256 ->   76 x  76 x 256 0.757 BF\n",
            "  55 conv    512       3 x 3/ 2     76 x  76 x 256 ->   38 x  38 x 512 3.407 BF\n",
            "  56 conv    256       1 x 1/ 1     38 x  38 x 512 ->   38 x  38 x 256 0.379 BF\n",
            "  57 route  55 \t\t                           ->   38 x  38 x 512 \n",
            "  58 conv    256       1 x 1/ 1     38 x  38 x 512 ->   38 x  38 x 256 0.379 BF\n",
            "  59 conv    256       1 x 1/ 1     38 x  38 x 256 ->   38 x  38 x 256 0.189 BF\n",
            "  60 conv    256       3 x 3/ 1     38 x  38 x 256 ->   38 x  38 x 256 1.703 BF\n",
            "  61 Shortcut Layer: 58,  wt = 0, wn = 0, outputs:  38 x  38 x 256 0.000 BF\n",
            "  62 conv    256       1 x 1/ 1     38 x  38 x 256 ->   38 x  38 x 256 0.189 BF\n",
            "  63 conv    256       3 x 3/ 1     38 x  38 x 256 ->   38 x  38 x 256 1.703 BF\n",
            "  64 Shortcut Layer: 61,  wt = 0, wn = 0, outputs:  38 x  38 x 256 0.000 BF\n",
            "  65 conv    256       1 x 1/ 1     38 x  38 x 256 ->   38 x  38 x 256 0.189 BF\n",
            "  66 conv    256       3 x 3/ 1     38 x  38 x 256 ->   38 x  38 x 256 1.703 BF\n",
            "  67 Shortcut Layer: 64,  wt = 0, wn = 0, outputs:  38 x  38 x 256 0.000 BF\n",
            "  68 conv    256       1 x 1/ 1     38 x  38 x 256 ->   38 x  38 x 256 0.189 BF\n",
            "  69 conv    256       3 x 3/ 1     38 x  38 x 256 ->   38 x  38 x 256 1.703 BF\n",
            "  70 Shortcut Layer: 67,  wt = 0, wn = 0, outputs:  38 x  38 x 256 0.000 BF\n",
            "  71 conv    256       1 x 1/ 1     38 x  38 x 256 ->   38 x  38 x 256 0.189 BF\n",
            "  72 conv    256       3 x 3/ 1     38 x  38 x 256 ->   38 x  38 x 256 1.703 BF\n",
            "  73 Shortcut Layer: 70,  wt = 0, wn = 0, outputs:  38 x  38 x 256 0.000 BF\n",
            "  74 conv    256       1 x 1/ 1     38 x  38 x 256 ->   38 x  38 x 256 0.189 BF\n",
            "  75 conv    256       3 x 3/ 1     38 x  38 x 256 ->   38 x  38 x 256 1.703 BF\n",
            "  76 Shortcut Layer: 73,  wt = 0, wn = 0, outputs:  38 x  38 x 256 0.000 BF\n",
            "  77 conv    256       1 x 1/ 1     38 x  38 x 256 ->   38 x  38 x 256 0.189 BF\n",
            "  78 conv    256       3 x 3/ 1     38 x  38 x 256 ->   38 x  38 x 256 1.703 BF\n",
            "  79 Shortcut Layer: 76,  wt = 0, wn = 0, outputs:  38 x  38 x 256 0.000 BF\n",
            "  80 conv    256       1 x 1/ 1     38 x  38 x 256 ->   38 x  38 x 256 0.189 BF\n",
            "  81 conv    256       3 x 3/ 1     38 x  38 x 256 ->   38 x  38 x 256 1.703 BF\n",
            "  82 Shortcut Layer: 79,  wt = 0, wn = 0, outputs:  38 x  38 x 256 0.000 BF\n",
            "  83 conv    256       1 x 1/ 1     38 x  38 x 256 ->   38 x  38 x 256 0.189 BF\n",
            "  84 route  83 56 \t                           ->   38 x  38 x 512 \n",
            "  85 conv    512       1 x 1/ 1     38 x  38 x 512 ->   38 x  38 x 512 0.757 BF\n",
            "  86 conv   1024       3 x 3/ 2     38 x  38 x 512 ->   19 x  19 x1024 3.407 BF\n",
            "  87 conv    512       1 x 1/ 1     19 x  19 x1024 ->   19 x  19 x 512 0.379 BF\n",
            "  88 route  86 \t\t                           ->   19 x  19 x1024 \n",
            "  89 conv    512       1 x 1/ 1     19 x  19 x1024 ->   19 x  19 x 512 0.379 BF\n",
            "  90 conv    512       1 x 1/ 1     19 x  19 x 512 ->   19 x  19 x 512 0.189 BF\n",
            "  91 conv    512       3 x 3/ 1     19 x  19 x 512 ->   19 x  19 x 512 1.703 BF\n",
            "  92 Shortcut Layer: 89,  wt = 0, wn = 0, outputs:  19 x  19 x 512 0.000 BF\n",
            "  93 conv    512       1 x 1/ 1     19 x  19 x 512 ->   19 x  19 x 512 0.189 BF\n",
            "  94 conv    512       3 x 3/ 1     19 x  19 x 512 ->   19 x  19 x 512 1.703 BF\n",
            "  95 Shortcut Layer: 92,  wt = 0, wn = 0, outputs:  19 x  19 x 512 0.000 BF\n",
            "  96 conv    512       1 x 1/ 1     19 x  19 x 512 ->   19 x  19 x 512 0.189 BF\n",
            "  97 conv    512       3 x 3/ 1     19 x  19 x 512 ->   19 x  19 x 512 1.703 BF\n",
            "  98 Shortcut Layer: 95,  wt = 0, wn = 0, outputs:  19 x  19 x 512 0.000 BF\n",
            "  99 conv    512       1 x 1/ 1     19 x  19 x 512 ->   19 x  19 x 512 0.189 BF\n",
            " 100 conv    512       3 x 3/ 1     19 x  19 x 512 ->   19 x  19 x 512 1.703 BF\n",
            " 101 Shortcut Layer: 98,  wt = 0, wn = 0, outputs:  19 x  19 x 512 0.000 BF\n",
            " 102 conv    512       1 x 1/ 1     19 x  19 x 512 ->   19 x  19 x 512 0.189 BF\n",
            " 103 route  102 87 \t                           ->   19 x  19 x1024 \n",
            " 104 conv   1024       1 x 1/ 1     19 x  19 x1024 ->   19 x  19 x1024 0.757 BF\n",
            " 105 conv    512       1 x 1/ 1     19 x  19 x1024 ->   19 x  19 x 512 0.379 BF\n",
            " 106 conv   1024       3 x 3/ 1     19 x  19 x 512 ->   19 x  19 x1024 3.407 BF\n",
            " 107 conv    512       1 x 1/ 1     19 x  19 x1024 ->   19 x  19 x 512 0.379 BF\n",
            " 108 max                5x 5/ 1     19 x  19 x 512 ->   19 x  19 x 512 0.005 BF\n",
            " 109 route  107 \t\t                           ->   19 x  19 x 512 \n",
            " 110 max                9x 9/ 1     19 x  19 x 512 ->   19 x  19 x 512 0.015 BF\n",
            " 111 route  107 \t\t                           ->   19 x  19 x 512 \n",
            " 112 max               13x13/ 1     19 x  19 x 512 ->   19 x  19 x 512 0.031 BF\n",
            " 113 route  112 110 108 107 \t                   ->   19 x  19 x2048 \n",
            " 114 conv    512       1 x 1/ 1     19 x  19 x2048 ->   19 x  19 x 512 0.757 BF\n",
            " 115 conv   1024       3 x 3/ 1     19 x  19 x 512 ->   19 x  19 x1024 3.407 BF\n",
            " 116 conv    512       1 x 1/ 1     19 x  19 x1024 ->   19 x  19 x 512 0.379 BF\n",
            " 117 conv    256       1 x 1/ 1     19 x  19 x 512 ->   19 x  19 x 256 0.095 BF\n",
            " 118 upsample                 2x    19 x  19 x 256 ->   38 x  38 x 256\n",
            " 119 route  85 \t\t                           ->   38 x  38 x 512 \n",
            " 120 conv    256       1 x 1/ 1     38 x  38 x 512 ->   38 x  38 x 256 0.379 BF\n",
            " 121 route  120 118 \t                           ->   38 x  38 x 512 \n",
            " 122 conv    256       1 x 1/ 1     38 x  38 x 512 ->   38 x  38 x 256 0.379 BF\n",
            " 123 conv    512       3 x 3/ 1     38 x  38 x 256 ->   38 x  38 x 512 3.407 BF\n",
            " 124 conv    256       1 x 1/ 1     38 x  38 x 512 ->   38 x  38 x 256 0.379 BF\n",
            " 125 conv    512       3 x 3/ 1     38 x  38 x 256 ->   38 x  38 x 512 3.407 BF\n",
            " 126 conv    256       1 x 1/ 1     38 x  38 x 512 ->   38 x  38 x 256 0.379 BF\n",
            " 127 conv    128       1 x 1/ 1     38 x  38 x 256 ->   38 x  38 x 128 0.095 BF\n",
            " 128 upsample                 2x    38 x  38 x 128 ->   76 x  76 x 128\n",
            " 129 route  54 \t\t                           ->   76 x  76 x 256 \n",
            " 130 conv    128       1 x 1/ 1     76 x  76 x 256 ->   76 x  76 x 128 0.379 BF\n",
            " 131 route  130 128 \t                           ->   76 x  76 x 256 \n",
            " 132 conv    128       1 x 1/ 1     76 x  76 x 256 ->   76 x  76 x 128 0.379 BF\n",
            " 133 conv    256       3 x 3/ 1     76 x  76 x 128 ->   76 x  76 x 256 3.407 BF\n",
            " 134 conv    128       1 x 1/ 1     76 x  76 x 256 ->   76 x  76 x 128 0.379 BF\n",
            " 135 conv    256       3 x 3/ 1     76 x  76 x 128 ->   76 x  76 x 256 3.407 BF\n",
            " 136 conv    128       1 x 1/ 1     76 x  76 x 256 ->   76 x  76 x 128 0.379 BF\n",
            " 137 conv    256       3 x 3/ 1     76 x  76 x 128 ->   76 x  76 x 256 3.407 BF\n",
            " 138 conv    255       1 x 1/ 1     76 x  76 x 256 ->   76 x  76 x 255 0.754 BF\n",
            " 139 yolo\n",
            "[yolo] params: iou loss: ciou (4), iou_norm: 0.07, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.20\n",
            "nms_kind: greedynms (1), beta = 0.600000 \n",
            " 140 route  136 \t\t                           ->   76 x  76 x 128 \n",
            " 141 conv    256       3 x 3/ 2     76 x  76 x 128 ->   38 x  38 x 256 0.852 BF\n",
            " 142 route  141 126 \t                           ->   38 x  38 x 512 \n",
            " 143 conv    256       1 x 1/ 1     38 x  38 x 512 ->   38 x  38 x 256 0.379 BF\n",
            " 144 conv    512       3 x 3/ 1     38 x  38 x 256 ->   38 x  38 x 512 3.407 BF\n",
            " 145 conv    256       1 x 1/ 1     38 x  38 x 512 ->   38 x  38 x 256 0.379 BF\n",
            " 146 conv    512       3 x 3/ 1     38 x  38 x 256 ->   38 x  38 x 512 3.407 BF\n",
            " 147 conv    256       1 x 1/ 1     38 x  38 x 512 ->   38 x  38 x 256 0.379 BF\n",
            " 148 conv    512       3 x 3/ 1     38 x  38 x 256 ->   38 x  38 x 512 3.407 BF\n",
            " 149 conv    255       1 x 1/ 1     38 x  38 x 512 ->   38 x  38 x 255 0.377 BF\n",
            " 150 yolo\n",
            "[yolo] params: iou loss: ciou (4), iou_norm: 0.07, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.10\n",
            "nms_kind: greedynms (1), beta = 0.600000 \n",
            " 151 route  147 \t\t                           ->   38 x  38 x 256 \n",
            " 152 conv    512       3 x 3/ 2     38 x  38 x 256 ->   19 x  19 x 512 0.852 BF\n",
            " 153 route  152 116 \t                           ->   19 x  19 x1024 \n",
            " 154 conv    512       1 x 1/ 1     19 x  19 x1024 ->   19 x  19 x 512 0.379 BF\n",
            " 155 conv   1024       3 x 3/ 1     19 x  19 x 512 ->   19 x  19 x1024 3.407 BF\n",
            " 156 conv    512       1 x 1/ 1     19 x  19 x1024 ->   19 x  19 x 512 0.379 BF\n",
            " 157 conv   1024       3 x 3/ 1     19 x  19 x 512 ->   19 x  19 x1024 3.407 BF\n",
            " 158 conv    512       1 x 1/ 1     19 x  19 x1024 ->   19 x  19 x 512 0.379 BF\n",
            " 159 conv   1024       3 x 3/ 1     19 x  19 x 512 ->   19 x  19 x1024 3.407 BF\n",
            " 160 conv    255       1 x 1/ 1     19 x  19 x1024 ->   19 x  19 x 255 0.189 BF\n",
            " 161 yolo\n",
            "[yolo] params: iou loss: ciou (4), iou_norm: 0.07, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.05\n",
            "nms_kind: greedynms (1), beta = 0.600000 \n",
            "Total BFLOPS 128.459 \n",
            "avg_outputs = 1068395 \n",
            " Allocate additional workspace_size = 52.43 MB \n",
            "Loading weights from yolov4.weights...\n",
            " seen 64, trained: 32032 K-images (500 Kilo-batches_64) \n",
            "Done! Loaded 162 layers from weights-file \n",
            " Detection layer: 139 - type = 28 \n",
            " Detection layer: 150 - type = 28 \n",
            " Detection layer: 161 - type = 28 \n",
            "data/person.jpg: Predicted in 54.031000 milli-seconds.\n",
            "dog: 99%\n",
            "person: 100%\n",
            "horse: 98%\n",
            "Unable to init server: Could not connect: Connection refused\n",
            "\n",
            "(predictions:2891): Gtk-\u001b[1;33mWARNING\u001b[0m **: \u001b[34m08:28:03.411\u001b[0m: cannot open display: \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dupZnYTf9HOG"
      },
      "source": [
        "## Setting up the dataset format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSPeyhOS9IQ-"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCQMGCUysVdL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}